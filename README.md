# Newton Method Revisited: Global Convergence Rates up to  $\mathcal {O}\left(k^{-3} \right)$\ for Stepsize Schedules and Linesearch Procedures

### Abstract
This paper investigates the global convergence of stepsized Newton methods for convex functions with HÃ¶lder continuous Hessians or third derivatives. We propose several simple stepsize schedules with fast global convergence guarantees, up to $\mathcal O\left( k^{-3} \right)$. For cases with multiple plausible smoothness parameterizations or an unknown smoothness constant, we introduce a stepsize linesearch and a backtracking procedure with provable convergence as if the optimal smoothness parameters were known in advance.
Additionally, we present strong convergence guarantees for the practically popular Newton method with exact linesearch.